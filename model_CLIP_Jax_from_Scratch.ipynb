{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2K2QBDMhez+9/HQ18rZH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdul-basit-ai/JAX_for_LLMs/blob/main/model_CLIP_Jax_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "DigBqcEc1zPA"
      },
      "outputs": [],
      "source": [
        "# !pip install -U \"jax[tpu]\"\n",
        "# !pip install git+https://github.com/google/flax.git\n",
        "# print(\"Done:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit, vmap\n",
        "from flax import nnx\n",
        "import optax\n",
        "from typing import Sequence, Union, Tuple, Optional, Any,  Dict\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BPmyZhjp1_es"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration"
      ],
      "metadata": {
        "id": "atMrVwkZIxsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "#Imports the dataclass decorator from the standard Python library.\n",
        "#This decorator automatically generates special methods based on the type-annotated class variables\n",
        "\n",
        "@dataclass(frozen=True)#Applies the decorator and sets the frozen parameter to True.Mean the class is immutable\n",
        "class CLIPConfig:\n",
        "\n",
        "    projection_dim: int = 512 #output embedding dim\n",
        "    # Vision Encoder (ViT-B/32)\n",
        "    vision_image_size: int = 224\n",
        "    vision_patch_size: int = 32\n",
        "    vision_num_layers: int = 12\n",
        "    vision_num_heads: int = 12\n",
        "    vision_hidden_size: int = 768 #input embedding\n",
        "    vision_mlp_dim: int = 3072\n",
        "    vision_dropout_rate: float = 0.0\n",
        "\n",
        "    # Text Encoder\n",
        "    text_vocab_size: int = 49408\n",
        "    text_max_position_embeddings: int = 77\n",
        "    text_num_layers: int = 12\n",
        "    text_num_heads: int = 12\n",
        "    text_hidden_size: int = 512\n",
        "    text_mlp_dim: int = 2048\n",
        "    text_dropout_rate: float = 0.0\n",
        "\n",
        "CONFIG = CLIPConfig()"
      ],
      "metadata": {
        "id": "kmZ5u7Fr2MZC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Block"
      ],
      "metadata": {
        "id": "qpw2_M7xI3UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nnx.Module):\n",
        "    #Following Pre Normalization Architecture\n",
        "    def __init__(self, embed_dim:int, num_heads:int, dropout_rate:float,*,rngs = nnx.Rngs):#Model Initialization\n",
        "        #Making sure that embed_dim is divisible by num_heads, * measn the rngs should be passed explicitly with name not only by position\n",
        "        assert embed_dim % num_heads == 0 , print(\"Make sure embed_dim divisible by num_head\")\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.rngs = rngs\n",
        "\n",
        "        #Layer Norm before Attention Block\n",
        "        self.norm = nnx.LayerNorm(embed_dim)\n",
        "\n",
        "        #QKV, We split it later for efficency and project it now\n",
        "        # Remember we will pass rngs where ever there will be weights and biases , cuz Jax doesn't do it autommatically\n",
        "        #So we have to pass it explicitlt to generate random numbers for weights etc\n",
        "        self.qkv = nnx.Linear(embed_dim,embed_dim * 3, rngs = rngs)\n",
        "\n",
        "        #OutputProject,  Combines heads back to get same Matrix\n",
        "        self.out_proj = nnx.Linear(embed_dim, embed_dim, rngs = rngs)\n",
        "\n",
        "        #Dropout\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def _attention(self, x: jax.Array, rngs: nnx.Rngs,deterministic: bool) -> jax.Array:#return type hint, Deterministic : False, dropout active, else otherwise\n",
        "        qkv_out = self.qkv(x) # Batch_size, Seq_len,Embed_dim * 3\n",
        "        q, k, v = jnp.split(qkv_out,3,axis=-1) # Now, 3 *(B,S,E) axis = -1 the split happens at first dimension, B\n",
        "\n",
        "        def rearrange_for_attention(tensor):#To reshape and Permute for MHA\n",
        "            # B,S,E --> B,S,N_H * H_D\n",
        "            tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], self.num_heads,self.head_dim)\n",
        "            return jnp.transpose((tensor, (0,2,1,3)))\n",
        "        q_heads = rearrange_for_attention(q)\n",
        "        k_heads = rearrange_for_attention(k)\n",
        "        v_heads = rearrange_for_attention(v)\n",
        "\n",
        "        attn_weights = jnp.matmul(q_heads,jnp.swapaxes(k_heads,-1,-1))\n",
        "        attn_weights = attn_weights/jnp.sqrt(self.head_dim)\n",
        "        attn_weights = jax.nn.softmax(attn_weights, axis = -1)\n",
        "        context = jnp.matmul(attn_weights,v_heads)\n",
        "\n",
        "        #Reshape to originial\n",
        "        context = jnp.transpose(context, (0,2,1,3))\n",
        "        #Concatenate Heads\n",
        "        context = context.reshape(context.shape[0], context[1],self.embed_dim)\n",
        "        output = self.out_proj(context)\n",
        "        output = self.dropout(output, rngs = rngs, deterministic = deterministic,)\n",
        "        return output\n",
        "\n",
        "        #Forward in Jax\n",
        "    def __call__(self, x:jax.Array, *, deterministic:bool=False,rngs = nnx.Rngs)-> jax.Array:\n",
        "        norm_x = self.norm(x)\n",
        "        attn_out = self._attention(norm_x,rngs,deterministic=deterministic)\n",
        "        residual = x + attn_out\n",
        "        return residual\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CW2SyxTd2MWy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward Block"
      ],
      "metadata": {
        "id": "rZY9_n5II-bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Block(nnx.Module):\n",
        "    def ___init__(self,mlp_dim:int,out_dim:int, dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        #expand dim\n",
        "        self.Linear1 = nnx.Linear(out_dim,mlp_dim,rngs=rngs)\n",
        "        #project to original\n",
        "        self.Linear2 = nnx.Linear(mlp_dim,out_dim,rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(self, x:jax.Array, *,deterministic:bool=False, rngs = nnx.Rngs)-> jax.Array:\n",
        "        x = self.Linear1(x)\n",
        "        x = nnx.gelu(x)\n",
        "        x= self.Linear2(x)\n",
        "        x = self.dropout(x, rngs = rngs, deterministic = deterministic)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yyZpQLRg2MTe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "FxMCY8EtJD4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Layer(nnx.Module):\n",
        "    def __init__(self,embed_dim:int,num_heads:int,mlp_dim:int,dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        self.attn = AttentionBlock(embed_dim,num_heads,dropout_rate,rngs=rngs)\n",
        "        self.mlp_norm = nnx.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP_Block(mlp_dim,embed_dim,dropout_rate,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x:jax.Array, *,deterministic:bool=False, rngs = nnx.Rngs)-> jax.Array:\n",
        "        x = self.attn(x,deterministic=deterministic,rngs=rngs)\n",
        "        norm_x = self.mlp_norm(x)\n",
        "        mlp_out = self.mlp(norm_x,deterministic=deterministic,rngs=rngs)\n",
        "        x = x + mlp_out\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RpwfEapTIINk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder(nnx.Module):\n",
        "    def __init__(self, embed_dim:int, num_layers:int, num_heads:int, mlp_dim:int, dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        #Stack encoder layers\n",
        "        self.layers:Sequence[Transformer_Encoder_Layer] = [Transformer_Encoder_Layer(embed_dim=embed_dim,num_heads=num_heads,\n",
        "                                                                                    mlp_dim=mlp_dim,dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "        self.final_norm = nnx.LayerNorm(embed_dim)\n",
        "\n",
        "    def __call__(self,x:jax.Array,*,deterministic:bool=False,rngs = nnx.Rngs)-> jax.Array:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,deterministic=deterministic,rngs=rngs)\n",
        "            x = self.final_norm(x)\n",
        "            return x\n"
      ],
      "metadata": {
        "id": "rqWuhNgRIFMK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Model - GPT type, Decoder Only"
      ],
      "metadata": {
        "id": "gyA382NmIWc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_Text_Encoder(nnx.Module):\n",
        "    def __init__(self, vocab_size: int,embed_dim: int,hidden_dim: int,\n",
        "                 num_layers: int,num_heads: int,max_position_embeddings: int,dropout_rate: float,\n",
        "                 *,rngs: nnx.Rngs):\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #Embeddings\n",
        "        self.token_embedding = nnx.Embed(vocab_size = vocab_size,num_features = hidden_dim, rngs = rngs)\n",
        "        #Positional Encoding (Learned)\n",
        "        self.positional_encoding = nnx.Param(jax,random.normal(rngs.params(), (max_position_embeddings, hidden_dim))* 0.02)\n",
        "\n",
        "        #Trasnformer Stack\n",
        "        self.transformer = Transformer_Encoder(embed_dim=hidden_dim, num_layers= num_layers, num_heads=num_heads,mlp_dim = hidden_dim*4,dropout_rate=dropout_rate, rngs=rngs)\n",
        "        self.final_projrction = nnx.Linear(hidden_dim,embed_dim,rngs=rngs)\n",
        "\n",
        "    def __call__(self, input_ids = jnp.ndarray, attention_mask : Optional[jnp.ndarray]=None,*,deterministic:bool=False, rngs = nnx.Rngs)->jnp.ndarray:\n",
        "        sequence_length = input_ids.shape[1]\n",
        "        token_embedd = self.token_embedding(input_ids)\n",
        "        if sequence_length > self.max_position_embeddings:\n",
        "            raise ValueError(\n",
        "                f\"Sequence length ({sequence_length}) exceeds max position embeddings \"\n",
        "                f\"({self.max_position_embeddings}).\" )\n",
        "        # Add positional embeddings to the token embeddings\n",
        "        # We slice the learned positional matrix to match the current sequence length.\n",
        "        positional_embeds = self.positional_encoding.value[:sequence_length,:]\n",
        "        hidden_states = token_embedd + positional_embeds\n",
        "\n",
        "        #Running through Transformer Stack\n",
        "        encoded_output = self.transformer(hidden_states,deterministic=deterministic,rngs=rngs)\n",
        "\n",
        "        #Pooling (The CLIP Text Pooling Strategy)\n",
        "        if attention_mask is not None:\n",
        "            # This works because the attention mask is typically 1 for tokens, 0 for padding.\n",
        "            eos_indices = jnp.sum(attention_mask, axis=-1) - 1\n",
        "            # Use jnp.take_along_axis for advanced indexing to get the EOS vector for each batch item\n",
        "            # The indices need to be reshaped to (Batch, 1, 1) to match dimensions for indexing\n",
        "            eos_indices = eos_indices[:, jnp.newaxis, jnp.newaxis]\n",
        "\n",
        "            # Extract the EOS vector from the sequence\n",
        "            pooled_output = jnp.take_along_axis(encoded_output, eos_indices, axis=1)\n",
        "            # Reshape from (Batch, 1, Hidden_Dim) to (Batch, Hidden_Dim)\n",
        "            pooled_output = jnp.squeeze(pooled_output, axis=1)\n",
        "        else:\n",
        "            # Fallback: Just take the last element if no mask is provided (simpler models)\n",
        "            pooled_output = encoded_output[:, -1, :]\n",
        "\n",
        "        # 4. Final Projection\n",
        "        # Map the pooled vector to the fixed-size latent space for contrastive learning\n",
        "        final_embedding = self.final_projection(pooled_output)\n",
        "\n",
        "        return final_embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "6Vqa4Pm4Lm2u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Model"
      ],
      "metadata": {
        "id": "yTdJYwwDIrlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patch_embeddings(nnx.Module):\n",
        "    def __init__(self, input_channels:int, patch_size:int,hidden_dim:int,*,rngs=nnx.Rngs):\n",
        "        #2--D convolution with kernel and stride equals to patch_size is standard way to perform patch embeddings in VIT\n",
        "        # It projects (patch_size x patch_size x channels) -> (hidden_dim) vector.\n",
        "        self.proj = nnx.Conv(int_features = input_channels,\n",
        "                             out_features=hidden_dim, kernel_size=(patch_size,patch_size),\n",
        "                             strides = (patch_size,patch_size), padding='VALID', rngs = rngs)\n",
        "\n",
        "        def __call__(self, x:jnp.ndarray)->jnp.ndarray:\n",
        "            x = self.proj(x) # Output shape: (Batch, Num_Patches_H, Num_Patches_W, Hidden_Dim)\n",
        "            batch_size, h, w, c = x.shape\n",
        "            x = x.reshape(batch_size, h*w,hidden_dim) # BTCH_Size, Seq_Len, Hidden_dim\n",
        "            return x"
      ],
      "metadata": {
        "id": "iuhN4XsQ_R-1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_Vision_Encoder(nnx.Module):\n",
        "    def __init__(self,image_size: int,\n",
        "                 patch_size: int,\n",
        "                 input_channels: int,  # e.g., 3 for RGB\n",
        "                 embed_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 num_layers: int,\n",
        "                 num_heads: int,\n",
        "                 dropout_rate: float,\n",
        "                 *,\n",
        "                 rngs: nnx.Rngs):\n",
        "        #Calculating number of patches and seq_len from it\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        sequence_length = num_patches + 1 # 1 for cls token at end\n",
        "\n",
        "        #Patching in action\n",
        "        self.patch_embedding = Patch_embeddings(input_channels=input_channels,patch_size=patch_size,hidden_dim=hidden_dim,rngs=rngs)\n",
        "        #Adding CLS learned paprameter, shape(1,1,hidden_dim)\n",
        "        self.cls_token=nnx.Param(jax.random.normal(rngs.param(), (1,1,hidden_dim))* 0.02)\n",
        "        #Positional Encoding that is learned also\n",
        "        self.positional_embedding = nnx.Param(nnx.random.normal(rngs.param(),(sequence_length,hidden_dim))*0.02)\n",
        "\n",
        "        #Transformer Stack\n",
        "        self.transformer = Transformer_Encoder(embed_dim=hidden_dim, num_layers= num_layers, num_heads=num_heads,mlp_dim = hidden_dim*4,dropout_rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "        #Projection Layer\n",
        "        self.final_projection = nnx.Linear(hidden_dim,embed_dim,rngs=rngs)\n",
        "\n",
        "    def __call__(self, pixel_values:jnp.ndarray,*,deterministic:bool=False,rngs = nnx.Rngs)->jnp.ndarray:\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        #Patching\n",
        "        patch_embeds = self.patch_embedding(pixel_values)\n",
        "        #CLS Token addition\n",
        "        cls_token = jnp.tile(self.cls_token.value,(batch_size,1,1))# it will match the size - > 1,1,H -> B,1,H\n",
        "        hidden_states = jnp.concatenate([cls_token,patch_embeds],axis=1)\n",
        "        #Positional Encoding\n",
        "        hidden_states += self.positional_embedding.value\n",
        "        #Transformer\n",
        "        encoded_output = self.transformer(hidden_states,deterministic=deterministic,rngs=rngs)\n",
        "        #Pooling\n",
        "        pooled_output = encoded_output[:,0,:] # extract the cls token, from first position, B,H\n",
        "        #Projection\n",
        "        final_embedding = self.final_projection(pooled_output)\n",
        "\n",
        "        return final_embedding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NzivcUWaDK82"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting both Models Together"
      ],
      "metadata": {
        "id": "JEaaMp-XJK5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_Model(nnx.Module):\n",
        "    def __init__(self,config:Dict,*,rngs:nnx.Rngs):\n",
        "        self.text_encoder = CLIP_Text_Encoder(\n",
        "            vocab_size=config['vocab_size'],\n",
        "            embed_dim=config['embed_dim'],\n",
        "            hidden_dim=config['text_hidden_dim'],\n",
        "            num_layers=config['text_layers'],\n",
        "            num_heads=config['text_heads'],\n",
        "            max_position_embeddings=config['max_position_embeddings'],\n",
        "            dropout_rate=config['dropout_rate'],\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        self.vision_encoder = CLIP_Vision_Encoder(\n",
        "            image_size=config['image_size'],\n",
        "            patch_size=config['patch_size'],\n",
        "            input_channels=config['input_channels'],\n",
        "            embed_dim=config['embed_dim'],\n",
        "            hidden_dim=config['vision_hidden_dim'],\n",
        "            num_layers=config['vision_layers'],\n",
        "            num_heads=config['vision_heads'],\n",
        "            dropout_rate=config['dropout_rate'],\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        #Logit Scale  (Temperature Parameter). Initial value corresponds to an initial temperature (tau) of exp(4.6) ~ 100\n",
        "        self.log_scale = nnx.Param(jnp.asarray(4,6))\n",
        "\n",
        "    def get_text_features(self,input_ids: jnp.ndarray,attention_mask: Optional[jnp.ndarray] = None,*,deterministic: bool = False,rngs: nnx.Rngs)-> jnp.ndarray:\n",
        "        text_features = self.text_encoder(input_ids=input_ids,attention_mask=attention_mask,deterministic=deterministic,rngs=rngs)\n",
        "\n",
        "    # Normalize the embeddings (L2 norm) - crucial for dot-product similarity\n",
        "    # text_features = text_features/jnp.linalg.norm(text_features,axis = -1, keepdima=True)\n",
        "        return text_features\n",
        "\n",
        "    def get_image_features(self,pixel_values: jnp.ndarray,*,deterministic: bool = False,rngs: nnx.Rngs) -> jnp.ndarray:\n",
        "        image_features = self.vision_encoder(pixel_values = pixel_values, deterministic = deterministic, rngs = rngs)\n",
        "        image_features = image_features/jnp.linalg.norm(image_features, axis = -1, keepdims = True)\n",
        "        return image_features\n",
        "\n",
        "    def __call__(self, input_ids: jnp.ndarray,pixel_values: jnp.ndarray,attention_mask: Optional[jnp.ndarray] = None,*,deterministic: bool = False,rngs: nnx.Rngs) -> jnp.ndarray:\n",
        "        #gettinng normalized embeddings\n",
        "        image_features = self.get_image_features(pixel_values, deterministic = deterministic, rngs=rngs)\n",
        "        text_features = self.text_features(input_ids, attention_mask, deterministic=deterministic, rnngs=rngs)\n",
        "\n",
        "        #Calculating Logit scale with temp included\n",
        "        logits_scale = jnp.exp(self.log_scale.value)\n",
        "\n",
        "        #Similarity, contrastive Step, dot products with all\n",
        "        logits = logits_scale * jnp.matmul(image_features,text_features.T)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Gp2_fDEpJPvX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JvYiJHmx_R7l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InBHLYkP_R46"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}