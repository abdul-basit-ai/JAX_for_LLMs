{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOMe4hIqtu/YwQ01Ly20ubd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdul-basit-ai/JAX_for_LLMs/blob/main/CLIP_Jax_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "DigBqcEc1zPA"
      },
      "outputs": [],
      "source": [
        "# !pip install -U \"jax[tpu]\"\n",
        "# !pip install git+https://github.com/google/flax.git\n",
        "# print(\"Done:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit, vmap\n",
        "from flax import nnx\n",
        "import optax\n",
        "from typing import Sequence, Union, Tuple, Optional, Any\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BPmyZhjp1_es"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "#Imports the dataclass decorator from the standard Python library.\n",
        "#This decorator automatically generates special methods based on the type-annotated class variables\n",
        "\n",
        "@dataclass(frozen=True)#Applies the decorator and sets the frozen parameter to True.Mean the class is immutable\n",
        "class CLIPConfig:\n",
        "\n",
        "    projection_dim: int = 512 #output embedding dim\n",
        "    # Vision Encoder (ViT-B/32)\n",
        "    vision_image_size: int = 224\n",
        "    vision_patch_size: int = 32\n",
        "    vision_num_layers: int = 12\n",
        "    vision_num_heads: int = 12\n",
        "    vision_hidden_size: int = 768 #input embedding\n",
        "    vision_mlp_dim: int = 3072\n",
        "    vision_dropout_rate: float = 0.0\n",
        "\n",
        "    # Text Encoder\n",
        "    text_vocab_size: int = 49408\n",
        "    text_max_position_embeddings: int = 77\n",
        "    text_num_layers: int = 12\n",
        "    text_num_heads: int = 12\n",
        "    text_hidden_size: int = 512\n",
        "    text_mlp_dim: int = 2048\n",
        "    text_dropout_rate: float = 0.0\n",
        "\n",
        "CONFIG = CLIPConfig()"
      ],
      "metadata": {
        "id": "kmZ5u7Fr2MZC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nnx.Module):\n",
        "    #Following Pre Normalization Architecture\n",
        "    def __init__(self, embed_dim:int, num_heads:int, dropout_rate:float,*,rngs = nnx.Rngs):#Model Initialization\n",
        "        #Making sure that embed_dim is divisible by num_heads, * measn the rngs should be passed explicitly with name not only by position\n",
        "        assert embed_dim % num_heads == 0 , print(\"Make sure embed_dim divisible by num_head\")\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.rngs = rngs\n",
        "\n",
        "        #Layer Norm before Attention Block\n",
        "        self.norm = nnx.LayerNorm(embed_dim)\n",
        "\n",
        "        #QKV, We split it later for efficency and project it now\n",
        "        # Remember we will pass rngs where ever there will be weights and biases , cuz Jax doesn't do it autommatically\n",
        "        #So we have to pass it explicitlt to generate random numbers for weights etc\n",
        "        self.qkv = nnx.Linear(embed_dim,embed_dim * 3, rngs = rngs)\n",
        "\n",
        "        #OutputProject,  Combines heads back to get same Matrix\n",
        "        self.out_proj = nnx.Linear(embed_dim, embed_dim, rngs = rngs)\n",
        "\n",
        "        #Dropout\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def _attention(self, x: jax.Array, rngs: nnx.Rngs,deterministic: bool) -> jax.Array:#return type hint, Deterministic : False, dropout active, else otherwise\n",
        "        qkv_out = self.qkv(x) # Batch_size, Seq_len,Embed_dim * 3\n",
        "        q, k, v = jnp.split(qkv_out,3,axis=-1) # Now, 3 *(B,S,E) axis = -1 the split happens at first dimension, B\n",
        "\n",
        "        def rearrange_for_attention(tensor):#To reshape and Permute for MHA\n",
        "            # B,S,E --> B,S,N_H * H_D\n",
        "            tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], self.num_heads,self.head_dim)\n",
        "            return jnp.transpose((tensor, (0,2,1,3)))\n",
        "        q_heads = rearrange_for_attention(q)\n",
        "        k_heads = rearrange_for_attention(k)\n",
        "        v_heads = rearrange_for_attention(v)\n",
        "\n",
        "        attn_weights = jnp.matmul(q_heads,jnp.swapaxes(k_heads,-1,-1))\n",
        "        attn_weights = attn_weights/jnp.sqrt(self.head_dim)\n",
        "        attn_weights = jax.nn.softmax(attn_weights, axis = -1)\n",
        "        context = jnp.matmul(attn_weights,v_heads)\n",
        "\n",
        "        #Reshape to originial\n",
        "        context = jnp.transpose(context, (0,2,1,3))\n",
        "        #Concatenate Heads\n",
        "        context = context.reshape(context.shape[0], context[1],self.embed_dim)\n",
        "        output = self.out_proj(context)\n",
        "        output = self.dropout(output, rngs = rngs, deterministic = deterministic,)\n",
        "        return output\n",
        "\n",
        "        #Forward in Jax\n",
        "    def __call__(self, x:jax.Array, *, deterministic:bool=False,rngs = nnx.Rngs)-> jax.Array:\n",
        "        norm_x = self.norm(x)\n",
        "        attn_out = self._attention(norm_x,rngs,deterministic=deterministic)\n",
        "        residual = x + attn_out\n",
        "        return residual\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CW2SyxTd2MWy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Block(nnx.Module):\n",
        "    def ___init__(self,mlp_dim:int,out_dim:int, dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        #expand dim\n",
        "        self.Linear1 = nnx.Linear(out_dim,mlp_dim,rngs=rngs)\n",
        "        #project to original\n",
        "        self.Linear2 = nnx.Linear(mlp_dim,out_dim,rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(self, x:jax.Array, *,deterministic:bool=False, rngs = nnx.Rngs)-> jax.Array:\n",
        "        x = self.Linear1(x)\n",
        "        x = nnx.gelu(x)\n",
        "        x= self.Linear2(x)\n",
        "        x = self.dropout(x, rngs = rngs, deterministic = deterministic)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yyZpQLRg2MTe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Layer(nnx.Module):\n",
        "    def __init__(self,embed_dim:int,num_heads:int,mlp_dim:int,dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        self.attn = AttentionBlock(embed_dim,num_heads,dropout_rate,rngs=rngs)\n",
        "        self.mlp_norm = nnx.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP_Block(mlp_dim,embed_dim,dropout_rate,rngs=rngs)\n",
        "\n",
        "    def __call__(self, x:jax.Array, *,deterministic:bool=False, rngs = nnx.Rngs)-> jax.Array:\n",
        "        x = self.attn(x,deterministic=deterministic,rngs=rngs)\n",
        "        norm_x = self.mlp_norm(x)\n",
        "        mlp_out = self.mlp(norm_x,deterministic=deterministic,rngs=rngs)\n",
        "        x = x + mlp_out\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RpwfEapTIINk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder(nnx.Module):\n",
        "    def __init__(self, embed_dim:int, num_layers:int, num_heads:int, mlp_dim:int, dropout_rate:float,*,rngs = nnx.Rngs):\n",
        "        #Stack encoder layers\n",
        "        self.layers:Sequence[Transformer_Encoder_Layer] = [Transformer_Encoder_Layer(embed_dim=embed_dim,num_heads=num_heads,\n",
        "                                                                                    mlp_dim=mlp_dim,dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "        self.final_norm = nnx.LayerNorm(embed_dim)\n",
        "\n",
        "    def __call__(self,x:jax.Array,*,deterministic:bool=False,rngs = nnx.Rngs)-> jax.Array:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,deterministic=deterministic,rngs=rngs)\n",
        "            x = self.final_norm(x)\n",
        "            return x\n",
        ""
      ],
      "metadata": {
        "id": "rqWuhNgRIFMK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP_Text_Encoder(nnx.Module):\n",
        "    def __init__(self, vocab_size: int,embed_dim: int,hidden_dim: int,\n",
        "                 num_layers: int,num_heads: int,max_position_embeddings: int,dropout_rate: float,\n",
        "                 *,rngs: nnx.Rngs):\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #Embeddings\n",
        "        self.token_embedding = nnx.Embed(vocab_size = vocab_size,num_features = hidden_dim, rngs = rngs)\n",
        "        #Positional Encoding (Learned)\n",
        "        self.positional_encoding = nnx.Param(jax,random.normal(rngs.params(), (max_position_embeddings, hidden_dim))* 0.02)\n",
        "\n",
        "        #Trasnformer Stack\n",
        "        self.transformer = Transformer_Encoder(embed_dim=hidden_dim, num_layers= num_layers, num_heads=num_heads,mlp_dim = hidden_dim*4,dropout_rate=dropout_rate, rngs=rngs)\n",
        "        self.final_projrction = nnx.Linear(hidden_dim,embed_dim,rngs=rngs)\n",
        "\n",
        "    def __call__(self, input_ids = jnp.ndarray, attention_mask : Optional[jnp.ndarray]=None,*,deterministic:bool=False, rngs = nnx.Rngs)->jnp.ndarray:\n",
        "        sequence_length = input_ids.shape[1]\n",
        "        token_embedd = self.token_embedding(input_ids)\n",
        "        if sequence_length > self.max_position_embeddings:\n",
        "            raise ValueError(\n",
        "                f\"Sequence length ({sequence_length}) exceeds max position embeddings \"\n",
        "                f\"({self.max_position_embeddings}).\" )\n",
        "        # Add positional embeddings to the token embeddings\n",
        "        # We slice the learned positional matrix to match the current sequence length.\n",
        "        positional_embeds = self.positional_encoding.value[:sequence_length,:]\n",
        "        hidden_states = token_embedd + positional_embeds\n",
        "\n",
        "        #Running through Transformer Stack\n",
        "        encoded_output = self.transformer(hidden_states,deterministic=deterministic,rngs=rngs)\n",
        "\n",
        "        #Pooling (The CLIP Text Pooling Strategy)\n",
        "        if attention_mask is not None:\n",
        "            # This works because the attention mask is typically 1 for tokens, 0 for padding.\n",
        "            eos_indices = jnp.sum(attention_mask, axis=-1) - 1\n",
        "            # Use jnp.take_along_axis for advanced indexing to get the EOS vector for each batch item\n",
        "            # The indices need to be reshaped to (Batch, 1, 1) to match dimensions for indexing\n",
        "            eos_indices = eos_indices[:, jnp.newaxis, jnp.newaxis]\n",
        "\n",
        "            # Extract the EOS vector from the sequence\n",
        "            pooled_output = jnp.take_along_axis(encoded_output, eos_indices, axis=1)\n",
        "            # Reshape from (Batch, 1, Hidden_Dim) to (Batch, Hidden_Dim)\n",
        "            pooled_output = jnp.squeeze(pooled_output, axis=1)\n",
        "        else:\n",
        "            # Fallback: Just take the last element if no mask is provided (simpler models)\n",
        "            pooled_output = encoded_output[:, -1, :]\n",
        "\n",
        "        # 4. Final Projection\n",
        "        # Map the pooled vector to the fixed-size latent space for contrastive learning\n",
        "        final_embedding = self.final_projection(pooled_output)\n",
        "\n",
        "        return final_embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "6Vqa4Pm4Lm2u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add clip_vision_encoder.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1pp-C4mRMyR",
        "outputId": "49ca60ad-f53f-4f2c-fa78-870d508110f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}